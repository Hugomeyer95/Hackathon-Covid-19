{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "articles = {}\n",
    "stat = { }\n",
    "for dirpath, subdirs, files in os.walk(r'C:\\Users\\hugomeyer\\Documents\\Hackathon\\Input\\comm_use_subset\\comm_use_subset\\pdf_json'):\n",
    "    for x in files:\n",
    "        if x.endswith(\".json\"):\n",
    "            articles[x] = os.path.join(dirpath, x)        \n",
    "df = pd.read_csv(r'C:\\Users\\hugomeyer\\Documents\\Hackathon\\Input\\metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_ref = ['covid-19', 'coronavirus', 'cov-2', 'sars-cov-2', 'sars-cov', 'hcov', '2019-ncov']\n",
    "\n",
    "season =  [{'Spring': ['spring','flower', 'flowering', 'thawing', 'melting']},\n",
    "            {'Autumn': ['Autumn','Humid', 'rain']},\n",
    "            {'Winter': ['Winter', 'cold','solstice', 'polar']},\n",
    "            {'Summer': ['Summer', 'summery', 'wintry', 'overwinterin','dry','hot']},\n",
    "            {'Time': ['Time', 'Periodic', 'regular','cylce','year','annual','semester','bimester','calendar','day','night','week']},\n",
    "            {'Climate': ['weather', 'temperature', 'hot','cold','humid','dry','Moisture','rain','sun','light','wind','rain','snow','monsoon']}]\n",
    "\n",
    "socials = ['quarantine','gathering','outside','party','bar','restaurant','drinks','running','meeting','park','mobility','cafe','shopping center','theme park','museums','libraries','movie theaters','retail','recreation','beaches','marinas','dog parks','plaza','public garden','train station','subway','bus']\n",
    "higher_terms = ['over', 'above', 'higher', 'older', '>', 'over', 'less']\n",
    "lower_terms = ['under', 'below', 'fewer', 'younger', '<', 'under', 'more']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchers = {    \n",
    "    \"Term Matcher\": lambda term: [{'LOWER': t} for t in term.split(' ')],\n",
    "    \"Terms Matcher\": lambda terms: [{\"LOWER\": {\"IN\": terms } }],\n",
    "    \"Number Suffix Matcher\": lambda periods: [\n",
    "        {'LIKE_NUM': True},\n",
    "        {\"TEXT\": {\"REGEX\": f'({\"|\".join(periods)})'}}\n",
    "    ],\n",
    "    \"Number Interval Matcher\": lambda periods: [\n",
    "        {'POS': 'NUM',},\n",
    "        {'TEXT': {'REGEX': f'({\"|\".join(periods)})'}, 'OP': '?'},\n",
    "        {'DEP': 'quantmod', 'OP': '?'},\n",
    "        {'DEP': 'punct', 'OP': '?'},\n",
    "        {'DEP': 'prep', 'OP': '?'},\n",
    "        {'POS': 'NUM'},\n",
    "        {'TEXT': {'REGEX': f'({\"|\".join(periods)})'}},\n",
    "    ],\n",
    "    \"Group Matcher\": [\n",
    "        {\"TEXT\": {\"IN\": higher_terms+lower_terms }}\n",
    "    ]                 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dict(stat, t = 10, sort_values = False, barh = False, width = 20, height = 4, title = ''):\n",
    "    filtered = dict(stat)\n",
    "    to_delete = []\n",
    "    for key in filtered:\n",
    "        if filtered[key] < t:\n",
    "            to_delete.append(key)\n",
    "    for key in to_delete:\n",
    "        del filtered[key]\n",
    "\n",
    "    \n",
    "    if sort_values == False:\n",
    "        lists = sorted(filtered.items())\n",
    "    else:\n",
    "        if sort_values == True:\n",
    "            lists = sorted(filtered.items(), key = lambda item : item[1])\n",
    "        else:\n",
    "            lists = sorted(filtered.items(), key = sort_values)\n",
    "               \n",
    "    fig = figure(num=None, figsize=(width, height))\n",
    "    \n",
    "    if title != '':\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        \n",
    "    x, y = zip(*lists) \n",
    "    \n",
    "    if barh == True:\n",
    "        plt.barh(x, y)\n",
    "    else:\n",
    "        plt.bar(x, y)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def merge_keys(mergers, obj):\n",
    "    result = dict(obj)\n",
    "    for key, arr in mergers:\n",
    "        if key not in result:\n",
    "            result[key] = 0\n",
    "        for merger in arr:\n",
    "            if merger in result:\n",
    "                result[key] = result[key] + result[merger]\n",
    "                del result[merger]\n",
    "    return result\n",
    "\n",
    "def dict_counter(res, arg):\n",
    "    try:\n",
    "        key = str(arg)\n",
    "        res.setdefault(key, 0)\n",
    "        res[key] = res[key] + 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def numval(val):\n",
    "    try:\n",
    "        return int(float(str(val))) \n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def day_value(val, rep = None):\n",
    "    \n",
    "    if rep != None:\n",
    "        val = numval(val.text)\n",
    "        if val != None and 'week' in rep.text:\n",
    "            val = val * 7\n",
    "        return val\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def report_interval(res, min_val, max_val):       \n",
    "    if min_val != None and max_val != None:\n",
    "        for key in range(min_val, max_val):\n",
    "            res.setdefault(key, 0)\n",
    "            res[key] = res[key] + 1    \n",
    "\n",
    "def virus_match(text):\n",
    "    return len(re.findall(rf'({\"|\".join(virus_ref)})', text, flags=re.IGNORECASE)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 47298/47298 [02:06<00:00, 375.09it/s]\n"
     ]
    }
   ],
   "source": [
    "literature = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    sha = str(row['sha'])\n",
    "    if sha != 'nan':\n",
    "        sha = sha + '.json';\n",
    "        try:\n",
    "            found = False\n",
    "            with open(articles[sha]) as f:\n",
    "                data = json.load(f)\n",
    "                #print([item['text'] for item in data['body_text']])\n",
    "                for key in ['abstract', 'body_text']:\n",
    "                    if found == False and key in data:\n",
    "                        for content in data[key]:\n",
    "                            text = content['text']\n",
    "                            if virus_match(text) == True:                                \n",
    "                                literature.append({'file': articles[sha], 'body': text})                                \n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15940\n"
     ]
    }
   ],
   "source": [
    "print(len(literature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_matches(match_arr, root, sentence, file, index = 0, execution = []):\n",
    "    key, result = match_arr[0]\n",
    "    rest = match_arr[1:]\n",
    "    next_exec = execution + [(key, result, index)]\n",
    "    if key in root:\n",
    "        rule = root[key]\n",
    "        if callable(rule):\n",
    "            rule( (result, next_exec, sentence, file) )            \n",
    "        else:\n",
    "            if 'execute' in rule:\n",
    "                rule['execute']( (result, next_exec, sentence, file) )\n",
    "            if len(rest) > 0:\n",
    "                execute_matches(rest, rule, sentence, file, index+1, next_exec)\n",
    "    \n",
    "    if len(rest) > 0:               \n",
    "        execute_matches(rest, root, sentence, file, index + 1, execution)\n",
    "        \n",
    "def merge_dict_values(original, rules, drop = []):\n",
    "    result = {}\n",
    "    arr_map = {}\n",
    "    for key, values in rules:\n",
    "        for val in values:\n",
    "            arr_map[val] = key\n",
    "    \n",
    "    for key in original.keys():\n",
    "        new_key = key if key not in arr_map else arr_map[key]        \n",
    "        if key not in drop and new_key not in drop:\n",
    "            val = original[key]            \n",
    "            result[new_key] = val if new_key not in result else result[new_key] + val\n",
    "            \n",
    "    return result\n",
    "    \n",
    "def merge_matches(matches, doc):\n",
    "    match_list = []\n",
    "    current = (None, None, None)\n",
    "    for match_id, start, end in matches:   \n",
    "        if match_id != current[0] or current[2] < start:\n",
    "            if current[0] != None:\n",
    "                match_list.append(current)\n",
    "            current = (match_id, start, end)\n",
    "        elif current[2] < end:\n",
    "            current = (match_id, current[1], end)\n",
    "        \n",
    "    match_list.append(current)\n",
    "    return match_list;\n",
    "\n",
    "def match_parser(matcher, doc, rule, file):\n",
    "    matches = matcher(doc)\n",
    "    if len(matches)>0:\n",
    "        to_process = []\n",
    "        for match_id, start, end in merge_matches(matches, doc):\n",
    "            string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "            span = doc[start:end]  # The matched span\n",
    "            to_process.append((string_id, span))\n",
    "        execute_matches(to_process, rule['root'], doc, file)\n",
    "\n",
    "def parse_body(matcher, text, rule, file = None, sentence_level = False):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    if sentence_level == True:    \n",
    "        for sent in doc.sents:\n",
    "            sent_doc = nlp(sent.text)\n",
    "            match_parser(matcher, sent_doc, rule, file)\n",
    "    else:\n",
    "        match_parser(matcher, doc, rule, file)\n",
    "\n",
    "def execute_ruleset(term, rule, sentence_level = False):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    for name, m in rule[\"Matchers\"]:\n",
    "        matcher.add(name, None, m)\n",
    "    print(len(literature))\n",
    "    for article in tqdm(literature):\n",
    "#     for article in literature:\n",
    "        text_list = re.compile(\"\\. \").split(article['body'])\n",
    "        file = article['file']\n",
    "        for text in text_list:\n",
    "            '''\n",
    "            if callable(term):\n",
    "                allow = term(text)\n",
    "            else:\n",
    "                allow = term == None or term in text\n",
    "            if allow == True:\n",
    "                '''\n",
    "            parse_body(matcher, text, rule, file, sentence_level)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15940"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(literature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat['socials'] = {}\n",
    "\n",
    "def match(text):\n",
    "    if virus_match(text) == True:\n",
    "        return len(re.findall(rf'\\ ({\"|\".join(symptoms)})\\ ', text)) > 0\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def social(res):\n",
    "    ref, agregate, sentence, file = res\n",
    "    dict_counter(stat['socials'], ref.text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/15940 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 1/15940 [00:00<53:31,  4.96it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'symptoms'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-20514014d12c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'social'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mexecute_ruleset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymptom_match\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m#plot_dict(stat['socials'], 50, True, title = \"Social\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-01b52fd13c19>\u001b[0m in \u001b[0;36mexecute_ruleset\u001b[1;34m(term, rule, sentence_level)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mallow\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 '''\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mparse_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-90-01b52fd13c19>\u001b[0m in \u001b[0;36mparse_body\u001b[1;34m(matcher, text, rule, file, sentence_level)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mmatch_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mmatch_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexecute_ruleset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_level\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-01b52fd13c19>\u001b[0m in \u001b[0;36mmatch_parser\u001b[1;34m(matcher, doc, rule, file)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mspan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# The matched span\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mto_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mexecute_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_process\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'root'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_level\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-01b52fd13c19>\u001b[0m in \u001b[0;36mexecute_matches\u001b[1;34m(match_arr, root, sentence, file, index, execution)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mrule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mrule\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_exec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'execute'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-7a4ef0dd0d26>\u001b[0m in \u001b[0;36msymptom\u001b[1;34m(res)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msymptom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magregate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdict_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'symptoms'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m rule = {    \n",
      "\u001b[1;31mKeyError\u001b[0m: 'symptoms'"
     ]
    }
   ],
   "source": [
    "rule_social = {    \n",
    "    \"Matchers\": [      \n",
    "       (\"Social Reference\", matchers['Terms Matcher'](socials)),\n",
    "    ],\n",
    "    \"root\": {\n",
    "        \"Social Reference\": social\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def social_match(text):\n",
    "    return len(re.findall(r'social', text)) > 0\n",
    "\n",
    "execute_ruleset(symptom_match, rule)\n",
    "#plot_dict(stat['socials'], 50, True, title = \"Social\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
